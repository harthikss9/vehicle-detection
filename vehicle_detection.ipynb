{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0a5db3-530a-4729-9ff9-b28654d9a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torch import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import box_iou\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fc747b-bae6-4ca6-8bd9-80e635a76e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Convert (cx, cy, w, h) -> (xmin, ymin, xmax, ymax)\n",
    "def convert2RCNNbox(bbox):\n",
    "    \"\"\"\n",
    "    Convert bounding box from (center_x, center_y, width, height)\n",
    "    to (xmin, ymin, xmax, ymax) format for Faster R-CNN.\n",
    "    \n",
    "    bbox: [cx, cy, w, h] in absolute pixel values.\n",
    "    Returns: [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    xmin = bbox[0] - bbox[2] / 2\n",
    "    xmax = bbox[0] + bbox[2] / 2\n",
    "    ymin = bbox[1] - bbox[3] / 2\n",
    "    ymax = bbox[1] + bbox[3] / 2\n",
    "\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "\n",
    "# ✅ Convert (xmin, ymin, xmax, ymax) -> relative (cx, cy, w, h)\n",
    "def convert2Outputbox(bbox, width, height):\n",
    "    \"\"\"\n",
    "    Convert bounding box from (xmin, ymin, xmax, ymax) to relative (cx, cy, w, h).\n",
    "    \n",
    "    bbox: [xmin, ymin, xmax, ymax] in absolute pixel values.\n",
    "    width, height: Dimensions of the image.\n",
    "    Returns: (cx, cy, w, h) as relative values.\n",
    "    \"\"\"\n",
    "    w = bbox[2] - bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "    cx = (bbox[0] + w / 2) / width\n",
    "    cy = (bbox[1] + h / 2) / height\n",
    "    w /= width\n",
    "    h /= height\n",
    "\n",
    "    return cx, cy, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34277099-be29-4b66-8cb8-4feaf998162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDetection(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, transforms) :\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "\n",
    "    self.imgs = list(sorted(glob.glob(root+\"images/**.jpeg\")))\n",
    "\n",
    "    with open(os.path.join(root,\"labels.txt\")) as f:\n",
    "      lbs = [x.split() for x in f.read().strip().splitlines()]\n",
    "\n",
    "    labels_map = {}\n",
    "    for x in lbs:\n",
    "      if labels_map.get(x[0]) != None:\n",
    "        labels_map[x[0]] += \",\" + \" \".join(x[1:])\n",
    "      else:\n",
    "        labels_map[x[0]] = \" \".join(x[1:])\n",
    "    self.labels = [x[1] for x in labels_map.items()]\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "    img = read_image(img_path)\n",
    "    img = tv_tensors.Image(img)\n",
    "\n",
    "    lb = [x.split() for x in self.labels[idx].split(\",\")]\n",
    "    classes = np.array([x[0] for x in lb], dtype=np.int64)\n",
    "    boxes = [convert2RCNNbox(np.array(x[1:], dtype=np.float32)) for x in lb]\n",
    "\n",
    "    target = {}\n",
    "    target[\"labels\"] = torch.from_numpy(classes)\n",
    "    target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
    "    target[\"image_id\"] = idx\n",
    "\n",
    "    if self.transforms is not None:\n",
    "        img, target = self.transforms(img, target)\n",
    "\n",
    "    return img, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "      im, target = zip(*batch)\n",
    "      return im, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aeb1d89-8a35-4508-9765-99ea3c4208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDetectionTest(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, transforms) :\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.imgs = list(sorted(glob.glob(root+\"images/**.jpeg\")))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.root, \"images\",self.imgs[idx])\n",
    "    img = read_image(img_path)\n",
    "\n",
    "    img = tv_tensors.Image(img)\n",
    "\n",
    "    target = {}\n",
    "    target[\"image_id\"] = idx\n",
    "\n",
    "    if self.transforms is not None:\n",
    "        img, target = self.transforms(img, target)\n",
    "\n",
    "    return img, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)\n",
    "\n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "      im, target = zip(*batch)\n",
    "      return im, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165f6c79-048e-4768-b330-8549afbe52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    transforms = []\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb8441e-c7d5-4406-acf4-d9db8ebf822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classes = {}\n",
    "\n",
    "    for iteration, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = batch\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)  # Model returns a dictionary\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # ✅ Corrected loop\n",
    "        for key, loss in loss_dict.items():\n",
    "            loss_classes[key] = loss_classes.get(key, 0) + loss.item()  # Convert tensor to scalar\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 20 == 0:\n",
    "            print(f\"✅ Epoch {epoch} | Iteration {iteration} | Batch Loss: {losses.item()}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch} Completed | Total Loss: {total_loss}\")\n",
    "    return loss_classes, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09bfda85-730d-4fc9-89cf-5afc8740629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_loss(model, dataloader, device):\n",
    "    total_loss = 0\n",
    "    loss_classes = {}\n",
    "\n",
    "    for iteration, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "        # ✅ Corrected loop\n",
    "        for key, loss in loss_dict.items():\n",
    "            loss_classes[key] = loss_classes.get(key, 0) + loss.item()  # Convert tensor to scalar\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    return loss_classes, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89009df-018f-4254-86c8-0a3e89cb0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "  model.eval()\n",
    "  predictions = {}\n",
    "  total_inference_time = 0\n",
    "  for iteration, batch in enumerate(dataloader):\n",
    "    images, targets = batch\n",
    "    images = list(image.to(device) for image in images)\n",
    "\n",
    "    model_time = time.time()\n",
    "\n",
    "    with torch.no_grad():      #torch.no_grad() added to avoid OOM issue\n",
    "      outputs = model(images)\n",
    "\n",
    "    outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs]\n",
    "    res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
    "\n",
    "    model_time = time.time() - model_time\n",
    "    total_inference_time += model_time\n",
    "\n",
    "    predictions.update(res)\n",
    "\n",
    "  print(f'Inference time: {total_inference_time}')\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ee5ac4-e35d-491f-a5d3-64009f3ca9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float) -> torch.Tensor:\n",
    "    order = torch.argsort(-scores)\n",
    "    indices = torch.arange(bboxes.shape[0])\n",
    "    keep = torch.ones_like(indices, dtype=torch.bool)\n",
    "    for i in indices:\n",
    "        if keep[i]:\n",
    "            bbox = bboxes[order[i]]\n",
    "            iou = box_iou(bbox[None,...],(bboxes[order[i + 1:]]) * keep[i + 1:][...,None])\n",
    "            overlapped = torch.nonzero(iou > iou_threshold)\n",
    "            keep[overlapped + i + 1] = 0\n",
    "    return order[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f434493d-71bf-43af-8c8d-33d1ba878463",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/WAVE/projects/CSEN-342-Wi25/data/pr2/\"\n",
    "output_dir = \"/WAVE/users2/unix/ssonpole/pr2/\"  # Or any directory where you can save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bac5242-2ac0-43ed-827c-475d461ee3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "\n",
    "# ✅ Device Configuration\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# ✅ Define Model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=\"DEFAULT\", trainable_backbone_layers=3\n",
    ")\n",
    "\n",
    "num_classes = 4  # 3 vehicle classes (1-3) + 1 background (0)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# ✅ Move Model to Device\n",
    "model.to(device)\n",
    "\n",
    "# ✅ Load Datasets\n",
    "root = \"/WAVE/projects/CSEN-342-Wi25/data/pr2/\"\n",
    "output_dir = \"/WAVE/users2/unix/ssonpole/pr2/\"\n",
    "\n",
    "dataset = VehicleDetection(root + \"train/\", get_transform())\n",
    "dataset_val = VehicleDetection(root + \"val/\", get_transform())\n",
    "dataset_test = VehicleDetectionTest(root + \"test/\", get_transform())\n",
    "\n",
    "# ✅ Data Loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=VehicleDetection.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=2, shuffle=False, num_workers=2, collate_fn=VehicleDetection.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=2, collate_fn=VehicleDetection.collate_fn\n",
    ")\n",
    "\n",
    "# ✅ Optimizer & Scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a37854-c96d-4b2e-a9f0-82c85ef9dcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 0 | Iteration 0 | Batch Loss: 2.1353402137756348\n",
      "✅ Epoch 0 | Iteration 20 | Batch Loss: 0.30748146772384644\n",
      "✅ Epoch 0 | Iteration 40 | Batch Loss: 0.15795676410198212\n",
      "✅ Epoch 0 | Iteration 60 | Batch Loss: 0.5511859059333801\n",
      "✅ Epoch 0 | Iteration 80 | Batch Loss: 0.18908871710300446\n",
      "✅ Epoch 0 | Iteration 100 | Batch Loss: 0.5248900055885315\n",
      "✅ Epoch 0 | Iteration 120 | Batch Loss: 0.2920415997505188\n",
      "✅ Epoch 0 | Iteration 140 | Batch Loss: 0.5767536163330078\n",
      "✅ Epoch 0 | Iteration 160 | Batch Loss: 0.3664097487926483\n",
      "✅ Epoch 0 | Iteration 180 | Batch Loss: 0.4992969036102295\n",
      "✅ Epoch 0 | Iteration 200 | Batch Loss: 0.1560649424791336\n",
      "✅ Epoch 0 | Iteration 220 | Batch Loss: 0.4665796458721161\n",
      "✅ Epoch 0 | Iteration 240 | Batch Loss: 0.3982652425765991\n",
      "✅ Epoch 0 | Iteration 260 | Batch Loss: 0.5172609090805054\n",
      "✅ Epoch 0 | Iteration 280 | Batch Loss: 0.37000057101249695\n",
      "✅ Epoch 0 | Iteration 300 | Batch Loss: 0.6724282503128052\n",
      "✅ Epoch 0 | Iteration 320 | Batch Loss: 0.45994019508361816\n",
      "✅ Epoch 0 | Iteration 340 | Batch Loss: 0.28164732456207275\n",
      "✅ Epoch 0 | Iteration 360 | Batch Loss: 0.4429991543292999\n",
      "✅ Epoch 0 | Iteration 380 | Batch Loss: 0.13629275560379028\n",
      "✅ Epoch 0 | Iteration 400 | Batch Loss: 0.44572150707244873\n",
      "✅ Epoch 0 | Iteration 420 | Batch Loss: 0.5367525815963745\n",
      "✅ Epoch 0 | Iteration 440 | Batch Loss: 0.2855508029460907\n",
      "✅ Epoch 0 | Iteration 460 | Batch Loss: 0.38888052105903625\n",
      "✅ Epoch 0 | Iteration 480 | Batch Loss: 0.3598848581314087\n",
      "✅ Epoch 0 | Iteration 500 | Batch Loss: 0.26747485995292664\n",
      "✅ Epoch 0 | Iteration 520 | Batch Loss: 0.4684857726097107\n",
      "✅ Epoch 0 | Iteration 540 | Batch Loss: 0.45586276054382324\n",
      "✅ Epoch 0 | Iteration 560 | Batch Loss: 0.2767314910888672\n",
      "✅ Epoch 0 | Iteration 580 | Batch Loss: 0.47996413707733154\n",
      "✅ Epoch 0 | Iteration 600 | Batch Loss: 0.38541799783706665\n",
      "✅ Epoch 0 | Iteration 620 | Batch Loss: 0.21522632241249084\n",
      "✅ Epoch 0 | Iteration 640 | Batch Loss: 0.5969274640083313\n",
      "✅ Epoch 0 | Iteration 660 | Batch Loss: 0.4452170431613922\n",
      "✅ Epoch 0 | Iteration 680 | Batch Loss: 0.4007834494113922\n",
      "✅ Epoch 0 | Iteration 700 | Batch Loss: 0.2576060891151428\n",
      "✅ Epoch 0 | Iteration 720 | Batch Loss: 0.3387635350227356\n",
      "✅ Epoch 0 | Iteration 740 | Batch Loss: 0.3708080053329468\n",
      "✅ Epoch 0 | Iteration 760 | Batch Loss: 0.3744175434112549\n",
      "✅ Epoch 0 | Iteration 780 | Batch Loss: 0.1944500058889389\n",
      "✅ Epoch 0 | Iteration 800 | Batch Loss: 0.36601966619491577\n",
      "✅ Epoch 0 | Iteration 820 | Batch Loss: 0.5500121712684631\n",
      "✅ Epoch 0 | Iteration 840 | Batch Loss: 0.3391783535480499\n",
      "✅ Epoch 0 | Iteration 860 | Batch Loss: 0.2763349413871765\n",
      "✅ Epoch 0 | Iteration 880 | Batch Loss: 0.2475557178258896\n",
      "✅ Epoch 0 | Iteration 900 | Batch Loss: 0.3163873553276062\n",
      "✅ Epoch 0 | Iteration 920 | Batch Loss: 0.40123075246810913\n",
      "✅ Epoch 0 | Iteration 940 | Batch Loss: 0.4781973659992218\n",
      "✅ Epoch 0 | Iteration 960 | Batch Loss: 0.38429152965545654\n",
      "✅ Epoch 0 | Iteration 980 | Batch Loss: 0.24316000938415527\n",
      "✅ Epoch 0 | Iteration 1000 | Batch Loss: 0.27583402395248413\n",
      "✅ Epoch 0 | Iteration 1020 | Batch Loss: 0.30864840745925903\n",
      "✅ Epoch 0 | Iteration 1040 | Batch Loss: 0.18600420653820038\n",
      "✅ Epoch 0 | Iteration 1060 | Batch Loss: 0.394702285528183\n",
      "✅ Epoch 0 | Iteration 1080 | Batch Loss: 0.22139714658260345\n",
      "✅ Epoch 0 | Iteration 1100 | Batch Loss: 0.3085740804672241\n",
      "✅ Epoch 0 | Iteration 1120 | Batch Loss: 0.36284857988357544\n",
      "✅ Epoch 0 | Iteration 1140 | Batch Loss: 0.19628645479679108\n",
      "✅ Epoch 0 | Iteration 1160 | Batch Loss: 0.381633996963501\n",
      "✅ Epoch 0 | Iteration 1180 | Batch Loss: 0.42619413137435913\n",
      "✅ Epoch 0 | Iteration 1200 | Batch Loss: 0.1981368511915207\n",
      "✅ Epoch 0 | Iteration 1220 | Batch Loss: 0.34845495223999023\n",
      "✅ Epoch 0 | Iteration 1240 | Batch Loss: 0.2552059292793274\n",
      "✅ Epoch 0 | Iteration 1260 | Batch Loss: 0.35230955481529236\n",
      "✅ Epoch 0 | Iteration 1280 | Batch Loss: 0.3969871401786804\n",
      "✅ Epoch 0 | Iteration 1300 | Batch Loss: 0.15986527502536774\n",
      "✅ Epoch 0 | Iteration 1320 | Batch Loss: 0.3768807649612427\n",
      "✅ Epoch 0 | Iteration 1340 | Batch Loss: 0.24148575961589813\n",
      "✅ Epoch 0 | Iteration 1360 | Batch Loss: 0.31591925024986267\n",
      "✅ Epoch 0 | Iteration 1380 | Batch Loss: 0.23134352266788483\n",
      "✅ Epoch 0 | Iteration 1400 | Batch Loss: 0.20323430001735687\n",
      "✅ Epoch 0 | Iteration 1420 | Batch Loss: 0.14900225400924683\n",
      "✅ Epoch 0 | Iteration 1440 | Batch Loss: 0.17710734903812408\n",
      "✅ Epoch 0 | Iteration 1460 | Batch Loss: 0.32480546832084656\n",
      "✅ Epoch 0 | Iteration 1480 | Batch Loss: 0.28714901208877563\n",
      "✅ Epoch 0 | Iteration 1500 | Batch Loss: 0.7509188652038574\n",
      "✅ Epoch 0 | Iteration 1520 | Batch Loss: 0.3613911271095276\n",
      "✅ Epoch 0 | Iteration 1540 | Batch Loss: 0.24102836847305298\n",
      "✅ Epoch 0 | Iteration 1560 | Batch Loss: 0.1806500107049942\n",
      "✅ Epoch 0 | Iteration 1580 | Batch Loss: 0.28934386372566223\n",
      "✅ Epoch 0 | Iteration 1600 | Batch Loss: 0.5557054281234741\n",
      "✅ Epoch 0 | Iteration 1620 | Batch Loss: 0.23819880187511444\n",
      "✅ Epoch 0 | Iteration 1640 | Batch Loss: 0.5312461256980896\n",
      "✅ Epoch 0 | Iteration 1660 | Batch Loss: 0.33712512254714966\n",
      "✅ Epoch 0 | Iteration 1680 | Batch Loss: 0.3138089179992676\n",
      "✅ Epoch 0 | Iteration 1700 | Batch Loss: 0.4250859320163727\n",
      "✅ Epoch 0 | Iteration 1720 | Batch Loss: 0.5677152276039124\n",
      "✅ Epoch 0 | Iteration 1740 | Batch Loss: 0.62843918800354\n",
      "✅ Epoch 0 | Iteration 1760 | Batch Loss: 0.8508232831954956\n",
      "✅ Epoch 0 | Iteration 1780 | Batch Loss: 0.24691402912139893\n",
      "✅ Epoch 0 | Iteration 1800 | Batch Loss: 0.401739239692688\n",
      "✅ Epoch 0 | Iteration 1820 | Batch Loss: 0.38089895248413086\n",
      "✅ Epoch 0 | Iteration 1840 | Batch Loss: 0.7830769419670105\n",
      "✅ Epoch 0 | Iteration 1860 | Batch Loss: 0.29129916429519653\n",
      "✅ Epoch 0 | Iteration 1880 | Batch Loss: 0.27224573493003845\n",
      "✅ Epoch 0 | Iteration 1900 | Batch Loss: 0.20757362246513367\n",
      "✅ Epoch 0 | Iteration 1920 | Batch Loss: 0.23341263830661774\n",
      "✅ Epoch 0 | Iteration 1940 | Batch Loss: 0.2571941912174225\n",
      "✅ Epoch 0 | Iteration 1960 | Batch Loss: 0.2806874215602875\n",
      "✅ Epoch 0 | Iteration 1980 | Batch Loss: 0.34054797887802124\n",
      "✅ Epoch 0 | Iteration 2000 | Batch Loss: 0.255344420671463\n",
      "✅ Epoch 0 | Iteration 2020 | Batch Loss: 0.3999876081943512\n",
      "✅ Epoch 0 | Iteration 2040 | Batch Loss: 0.2369215488433838\n",
      "✅ Epoch 0 | Iteration 2060 | Batch Loss: 0.2819429039955139\n",
      "✅ Epoch 0 | Iteration 2080 | Batch Loss: 0.2455369532108307\n",
      "✅ Epoch 0 | Iteration 2100 | Batch Loss: 0.160529226064682\n",
      "✅ Epoch 0 | Iteration 2120 | Batch Loss: 0.2891853153705597\n",
      "✅ Epoch 0 | Iteration 2140 | Batch Loss: 0.2359115183353424\n",
      "✅ Epoch 0 | Iteration 2160 | Batch Loss: 0.37094464898109436\n",
      "✅ Epoch 0 | Iteration 2180 | Batch Loss: 0.47748735547065735\n",
      "✅ Epoch 0 | Iteration 2200 | Batch Loss: 0.36227846145629883\n",
      "✅ Epoch 0 | Iteration 2220 | Batch Loss: 0.2669456899166107\n",
      "✅ Epoch 0 | Iteration 2240 | Batch Loss: 0.23421801626682281\n",
      "✅ Epoch 0 | Iteration 2260 | Batch Loss: 0.2773287892341614\n",
      "✅ Epoch 0 | Iteration 2280 | Batch Loss: 0.31145140528678894\n",
      "✅ Epoch 0 | Iteration 2300 | Batch Loss: 0.23973745107650757\n",
      "✅ Epoch 0 | Iteration 2320 | Batch Loss: 0.3101274371147156\n",
      "✅ Epoch 0 | Iteration 2340 | Batch Loss: 0.23157766461372375\n",
      "✅ Epoch 0 | Iteration 2360 | Batch Loss: 0.17233359813690186\n",
      "✅ Epoch 0 | Iteration 2380 | Batch Loss: 0.1911330372095108\n",
      "✅ Epoch 0 | Iteration 2400 | Batch Loss: 0.4018387794494629\n",
      "✅ Epoch 0 | Iteration 2420 | Batch Loss: 0.18930993974208832\n",
      "✅ Epoch 0 | Iteration 2440 | Batch Loss: 0.27553591132164\n",
      "✅ Epoch 0 | Iteration 2460 | Batch Loss: 0.40265336632728577\n",
      "✅ Epoch 0 | Iteration 2480 | Batch Loss: 0.4090845286846161\n",
      "✅ Epoch 0 | Iteration 2500 | Batch Loss: 0.47604385018348694\n",
      "✅ Epoch 0 | Iteration 2520 | Batch Loss: 0.2560100555419922\n",
      "✅ Epoch 0 | Iteration 2540 | Batch Loss: 0.5803695321083069\n",
      "✅ Epoch 0 | Iteration 2560 | Batch Loss: 0.32134827971458435\n",
      "✅ Epoch 0 | Iteration 2580 | Batch Loss: 0.11939593404531479\n",
      "✅ Epoch 0 | Iteration 2600 | Batch Loss: 0.29613471031188965\n",
      "✅ Epoch 0 | Iteration 2620 | Batch Loss: 0.49788331985473633\n",
      "✅ Epoch 0 | Iteration 2640 | Batch Loss: 0.37989386916160583\n",
      "✅ Epoch 0 | Iteration 2660 | Batch Loss: 0.2214924544095993\n",
      "✅ Epoch 0 | Iteration 2680 | Batch Loss: 0.31407108902931213\n",
      "✅ Epoch 0 | Iteration 2700 | Batch Loss: 0.39012765884399414\n",
      "✅ Epoch 0 | Iteration 2720 | Batch Loss: 0.40820106863975525\n",
      "✅ Epoch 0 | Iteration 2740 | Batch Loss: 0.07251746952533722\n",
      "✅ Epoch 0 | Iteration 2760 | Batch Loss: 0.22757588326931\n",
      "✅ Epoch 0 | Iteration 2780 | Batch Loss: 0.23392778635025024\n",
      "✅ Epoch 0 | Iteration 2800 | Batch Loss: 0.3197858929634094\n",
      "✅ Epoch 0 | Iteration 2820 | Batch Loss: 0.37990912795066833\n",
      "✅ Epoch 0 | Iteration 2840 | Batch Loss: 0.3503635823726654\n",
      "✅ Epoch 0 | Iteration 2860 | Batch Loss: 0.5258243083953857\n",
      "✅ Epoch 0 | Iteration 2880 | Batch Loss: 0.2917976677417755\n",
      "✅ Epoch 0 | Iteration 2900 | Batch Loss: 0.31795457005500793\n",
      "✅ Epoch 0 | Iteration 2920 | Batch Loss: 0.22994185984134674\n",
      "✅ Epoch 0 | Iteration 2940 | Batch Loss: 0.2544897794723511\n",
      "✅ Epoch 0 | Iteration 2960 | Batch Loss: 0.39463040232658386\n",
      "✅ Epoch 0 | Iteration 2980 | Batch Loss: 0.2703362703323364\n",
      "✅ Epoch 0 | Iteration 3000 | Batch Loss: 0.23336294293403625\n",
      "✅ Epoch 0 | Iteration 3020 | Batch Loss: 0.43484383821487427\n",
      "✅ Epoch 0 | Iteration 3040 | Batch Loss: 0.1873214840888977\n",
      "✅ Epoch 0 | Iteration 3060 | Batch Loss: 0.3881877362728119\n",
      "✅ Epoch 0 | Iteration 3080 | Batch Loss: 0.2475939840078354\n",
      "✅ Epoch 0 | Iteration 3100 | Batch Loss: 0.13074874877929688\n",
      "✅ Epoch 0 | Iteration 3120 | Batch Loss: 0.3194350302219391\n",
      "✅ Epoch 0 | Iteration 3140 | Batch Loss: 0.21143506467342377\n",
      "✅ Epoch 0 | Iteration 3160 | Batch Loss: 0.26630330085754395\n",
      "✅ Epoch 0 | Iteration 3180 | Batch Loss: 0.19322943687438965\n",
      "✅ Epoch 0 | Iteration 3200 | Batch Loss: 0.3596501052379608\n",
      "✅ Epoch 0 | Iteration 3220 | Batch Loss: 0.2082659751176834\n",
      "✅ Epoch 0 | Iteration 3240 | Batch Loss: 0.45755189657211304\n",
      "✅ Epoch 0 | Iteration 3260 | Batch Loss: 0.3454323709011078\n",
      "✅ Epoch 0 | Iteration 3280 | Batch Loss: 0.25849539041519165\n",
      "✅ Epoch 0 | Iteration 3300 | Batch Loss: 0.3991738557815552\n",
      "✅ Epoch 0 | Iteration 3320 | Batch Loss: 0.2213355004787445\n",
      "✅ Epoch 0 | Iteration 3340 | Batch Loss: 0.26238957047462463\n",
      "✅ Epoch 0 | Iteration 3360 | Batch Loss: 0.23394158482551575\n",
      "✅ Epoch 0 | Iteration 3380 | Batch Loss: 0.17138734459877014\n",
      "✅ Epoch 0 | Iteration 3400 | Batch Loss: 0.20339521765708923\n",
      "✅ Epoch 0 | Iteration 3420 | Batch Loss: 0.4456848204135895\n",
      "✅ Epoch 0 | Iteration 3440 | Batch Loss: 0.6906151175498962\n",
      "✅ Epoch 0 | Iteration 3460 | Batch Loss: 0.21601088345050812\n",
      "✅ Epoch 0 | Iteration 3480 | Batch Loss: 0.26489198207855225\n",
      "✅ Epoch 0 Completed | Total Loss: 1149.9898953326046\n",
      "✅ Saving the best model\n",
      "✅ Epoch 0 | Validation Loss: 290.8599950373173\n",
      "✅ Epoch 1 | Iteration 0 | Batch Loss: 0.2473633736371994\n",
      "✅ Epoch 1 | Iteration 20 | Batch Loss: 0.3038066029548645\n",
      "✅ Epoch 1 | Iteration 40 | Batch Loss: 0.08867879211902618\n",
      "✅ Epoch 1 | Iteration 60 | Batch Loss: 0.40585991740226746\n",
      "✅ Epoch 1 | Iteration 80 | Batch Loss: 0.11846713721752167\n",
      "✅ Epoch 1 | Iteration 100 | Batch Loss: 0.310631662607193\n",
      "✅ Epoch 1 | Iteration 120 | Batch Loss: 0.28094205260276794\n",
      "✅ Epoch 1 | Iteration 140 | Batch Loss: 0.16597916185855865\n",
      "✅ Epoch 1 | Iteration 160 | Batch Loss: 0.22481636703014374\n",
      "✅ Epoch 1 | Iteration 180 | Batch Loss: 0.17244520783424377\n",
      "✅ Epoch 1 | Iteration 200 | Batch Loss: 0.22819368541240692\n",
      "✅ Epoch 1 | Iteration 220 | Batch Loss: 0.18881891667842865\n",
      "✅ Epoch 1 | Iteration 240 | Batch Loss: 0.3609130084514618\n",
      "✅ Epoch 1 | Iteration 260 | Batch Loss: 0.4014791250228882\n",
      "✅ Epoch 1 | Iteration 280 | Batch Loss: 0.2834453582763672\n",
      "✅ Epoch 1 | Iteration 300 | Batch Loss: 0.18480531871318817\n",
      "✅ Epoch 1 | Iteration 320 | Batch Loss: 0.3214133381843567\n",
      "✅ Epoch 1 | Iteration 340 | Batch Loss: 0.4156138300895691\n",
      "✅ Epoch 1 | Iteration 360 | Batch Loss: 0.2881511449813843\n",
      "✅ Epoch 1 | Iteration 380 | Batch Loss: 0.29653316736221313\n",
      "✅ Epoch 1 | Iteration 400 | Batch Loss: 0.18132926523685455\n",
      "✅ Epoch 1 | Iteration 420 | Batch Loss: 0.3182622492313385\n",
      "✅ Epoch 1 | Iteration 440 | Batch Loss: 0.3026878237724304\n",
      "✅ Epoch 1 | Iteration 460 | Batch Loss: 0.3889717757701874\n",
      "✅ Epoch 1 | Iteration 480 | Batch Loss: 0.19401314854621887\n",
      "✅ Epoch 1 | Iteration 500 | Batch Loss: 0.2128312885761261\n",
      "✅ Epoch 1 | Iteration 520 | Batch Loss: 0.2978118658065796\n",
      "✅ Epoch 1 | Iteration 540 | Batch Loss: 0.19608846306800842\n",
      "✅ Epoch 1 | Iteration 560 | Batch Loss: 0.338328093290329\n",
      "✅ Epoch 1 | Iteration 580 | Batch Loss: 0.2043880820274353\n",
      "✅ Epoch 1 | Iteration 600 | Batch Loss: 0.1822512149810791\n",
      "✅ Epoch 1 | Iteration 620 | Batch Loss: 0.32731500267982483\n",
      "✅ Epoch 1 | Iteration 640 | Batch Loss: 0.3224294185638428\n",
      "✅ Epoch 1 | Iteration 660 | Batch Loss: 0.33691418170928955\n",
      "✅ Epoch 1 | Iteration 680 | Batch Loss: 0.2998240292072296\n",
      "✅ Epoch 1 | Iteration 700 | Batch Loss: 0.29132574796676636\n",
      "✅ Epoch 1 | Iteration 720 | Batch Loss: 0.4208414852619171\n",
      "✅ Epoch 1 | Iteration 740 | Batch Loss: 0.33427995443344116\n",
      "✅ Epoch 1 | Iteration 760 | Batch Loss: 0.20265793800354004\n",
      "✅ Epoch 1 | Iteration 780 | Batch Loss: 0.37675711512565613\n",
      "✅ Epoch 1 | Iteration 800 | Batch Loss: 0.2376386672258377\n",
      "✅ Epoch 1 | Iteration 820 | Batch Loss: 0.13913041353225708\n",
      "✅ Epoch 1 | Iteration 840 | Batch Loss: 0.33322760462760925\n",
      "✅ Epoch 1 | Iteration 860 | Batch Loss: 0.227743461728096\n",
      "✅ Epoch 1 | Iteration 880 | Batch Loss: 0.29578697681427\n",
      "✅ Epoch 1 | Iteration 900 | Batch Loss: 0.14472128450870514\n",
      "✅ Epoch 1 | Iteration 920 | Batch Loss: 0.44594618678092957\n",
      "✅ Epoch 1 | Iteration 940 | Batch Loss: 0.3060653507709503\n",
      "✅ Epoch 1 | Iteration 960 | Batch Loss: 0.22915932536125183\n",
      "✅ Epoch 1 | Iteration 980 | Batch Loss: 0.28632473945617676\n",
      "✅ Epoch 1 | Iteration 1000 | Batch Loss: 0.1374087631702423\n",
      "✅ Epoch 1 | Iteration 1020 | Batch Loss: 0.1627386063337326\n",
      "✅ Epoch 1 | Iteration 1040 | Batch Loss: 0.2812928557395935\n",
      "✅ Epoch 1 | Iteration 1060 | Batch Loss: 0.19477209448814392\n",
      "✅ Epoch 1 | Iteration 1080 | Batch Loss: 0.18338224291801453\n",
      "✅ Epoch 1 | Iteration 1100 | Batch Loss: 0.41445064544677734\n",
      "✅ Epoch 1 | Iteration 1120 | Batch Loss: 0.48411044478416443\n",
      "✅ Epoch 1 | Iteration 1140 | Batch Loss: 0.638094425201416\n",
      "✅ Epoch 1 | Iteration 1160 | Batch Loss: 0.2948559522628784\n",
      "✅ Epoch 1 | Iteration 1180 | Batch Loss: 0.15644524991512299\n",
      "✅ Epoch 1 | Iteration 1200 | Batch Loss: 0.470298707485199\n",
      "✅ Epoch 1 | Iteration 1220 | Batch Loss: 0.2559754550457001\n",
      "✅ Epoch 1 | Iteration 1240 | Batch Loss: 0.25678038597106934\n",
      "✅ Epoch 1 | Iteration 1260 | Batch Loss: 0.4309890568256378\n",
      "✅ Epoch 1 | Iteration 1280 | Batch Loss: 0.3213358521461487\n",
      "✅ Epoch 1 | Iteration 1300 | Batch Loss: 0.2951837182044983\n",
      "✅ Epoch 1 | Iteration 1320 | Batch Loss: 0.11553559452295303\n",
      "✅ Epoch 1 | Iteration 1340 | Batch Loss: 0.267902135848999\n",
      "✅ Epoch 1 | Iteration 1360 | Batch Loss: 0.1309865564107895\n",
      "✅ Epoch 1 | Iteration 1380 | Batch Loss: 0.28404727578163147\n",
      "✅ Epoch 1 | Iteration 1400 | Batch Loss: 0.5418570041656494\n",
      "✅ Epoch 1 | Iteration 1420 | Batch Loss: 0.28079336881637573\n",
      "✅ Epoch 1 | Iteration 1440 | Batch Loss: 0.26817917823791504\n",
      "✅ Epoch 1 | Iteration 1460 | Batch Loss: 0.33050113916397095\n",
      "✅ Epoch 1 | Iteration 1480 | Batch Loss: 0.24339604377746582\n",
      "✅ Epoch 1 | Iteration 1500 | Batch Loss: 0.14841032028198242\n",
      "✅ Epoch 1 | Iteration 1520 | Batch Loss: 0.25404953956604004\n",
      "✅ Epoch 1 | Iteration 1540 | Batch Loss: 0.5474674701690674\n",
      "✅ Epoch 1 | Iteration 1560 | Batch Loss: 0.24525436758995056\n",
      "✅ Epoch 1 | Iteration 1580 | Batch Loss: 0.2664521634578705\n",
      "✅ Epoch 1 | Iteration 1600 | Batch Loss: 0.26990729570388794\n",
      "✅ Epoch 1 | Iteration 1620 | Batch Loss: 0.2748667001724243\n",
      "✅ Epoch 1 | Iteration 1640 | Batch Loss: 0.1610691398382187\n",
      "✅ Epoch 1 | Iteration 1660 | Batch Loss: 0.10853536427021027\n",
      "✅ Epoch 1 | Iteration 1680 | Batch Loss: 0.16606855392456055\n",
      "✅ Epoch 1 | Iteration 1700 | Batch Loss: 0.13057777285575867\n",
      "✅ Epoch 1 | Iteration 1720 | Batch Loss: 0.31948140263557434\n",
      "✅ Epoch 1 | Iteration 1740 | Batch Loss: 0.3039003014564514\n",
      "✅ Epoch 1 | Iteration 1760 | Batch Loss: 0.13371053338050842\n",
      "✅ Epoch 1 | Iteration 1780 | Batch Loss: 0.37947598099708557\n",
      "✅ Epoch 1 | Iteration 1800 | Batch Loss: 0.2956857681274414\n",
      "✅ Epoch 1 | Iteration 1820 | Batch Loss: 0.24854135513305664\n",
      "✅ Epoch 1 | Iteration 1840 | Batch Loss: 0.20331895351409912\n",
      "✅ Epoch 1 | Iteration 1860 | Batch Loss: 0.19346678256988525\n",
      "✅ Epoch 1 | Iteration 1880 | Batch Loss: 0.42889103293418884\n",
      "✅ Epoch 1 | Iteration 1900 | Batch Loss: 0.2592703700065613\n",
      "✅ Epoch 1 | Iteration 1920 | Batch Loss: 0.5449748635292053\n",
      "✅ Epoch 1 | Iteration 1940 | Batch Loss: 0.27023595571517944\n",
      "✅ Epoch 1 | Iteration 1960 | Batch Loss: 0.06128808856010437\n",
      "✅ Epoch 1 | Iteration 1980 | Batch Loss: 0.22843484580516815\n",
      "✅ Epoch 1 | Iteration 2000 | Batch Loss: 0.13546697795391083\n",
      "✅ Epoch 1 | Iteration 2020 | Batch Loss: 0.5874770283699036\n",
      "✅ Epoch 1 | Iteration 2040 | Batch Loss: 0.3603878915309906\n",
      "✅ Epoch 1 | Iteration 2060 | Batch Loss: 0.2717112600803375\n",
      "✅ Epoch 1 | Iteration 2080 | Batch Loss: 0.4500216543674469\n",
      "✅ Epoch 1 | Iteration 2100 | Batch Loss: 0.24072617292404175\n",
      "✅ Epoch 1 | Iteration 2120 | Batch Loss: 0.3629918694496155\n",
      "✅ Epoch 1 | Iteration 2140 | Batch Loss: 0.3240126669406891\n",
      "✅ Epoch 1 | Iteration 2160 | Batch Loss: 0.10032162070274353\n",
      "✅ Epoch 1 | Iteration 2180 | Batch Loss: 0.17994597554206848\n",
      "✅ Epoch 1 | Iteration 2200 | Batch Loss: 0.1956690549850464\n",
      "✅ Epoch 1 | Iteration 2220 | Batch Loss: 0.1281810849905014\n",
      "✅ Epoch 1 | Iteration 2240 | Batch Loss: 0.13727055490016937\n",
      "✅ Epoch 1 | Iteration 2260 | Batch Loss: 0.45513075590133667\n",
      "✅ Epoch 1 | Iteration 2280 | Batch Loss: 0.26328539848327637\n",
      "✅ Epoch 1 | Iteration 2300 | Batch Loss: 0.3399330675601959\n",
      "✅ Epoch 1 | Iteration 2320 | Batch Loss: 0.242317333817482\n",
      "✅ Epoch 1 | Iteration 2340 | Batch Loss: 0.43955832719802856\n",
      "✅ Epoch 1 | Iteration 2360 | Batch Loss: 0.28435710072517395\n",
      "✅ Epoch 1 | Iteration 2380 | Batch Loss: 0.2948401868343353\n",
      "✅ Epoch 1 | Iteration 2400 | Batch Loss: 0.21195659041404724\n",
      "✅ Epoch 1 | Iteration 2420 | Batch Loss: 0.2163170576095581\n",
      "✅ Epoch 1 | Iteration 2440 | Batch Loss: 0.16670715808868408\n",
      "✅ Epoch 1 | Iteration 2460 | Batch Loss: 0.21982571482658386\n",
      "✅ Epoch 1 | Iteration 2480 | Batch Loss: 0.18660259246826172\n",
      "✅ Epoch 1 | Iteration 2500 | Batch Loss: 0.18388192355632782\n",
      "✅ Epoch 1 | Iteration 2520 | Batch Loss: 0.46372634172439575\n",
      "✅ Epoch 1 | Iteration 2540 | Batch Loss: 0.3729103207588196\n",
      "✅ Epoch 1 | Iteration 2560 | Batch Loss: 0.24746651947498322\n",
      "✅ Epoch 1 | Iteration 2580 | Batch Loss: 0.5549013614654541\n",
      "✅ Epoch 1 | Iteration 2600 | Batch Loss: 0.48916178941726685\n",
      "✅ Epoch 1 | Iteration 2620 | Batch Loss: 0.2922278046607971\n",
      "✅ Epoch 1 | Iteration 2640 | Batch Loss: 0.3062787353992462\n",
      "✅ Epoch 1 | Iteration 2660 | Batch Loss: 0.19735565781593323\n",
      "✅ Epoch 1 | Iteration 2680 | Batch Loss: 0.552959144115448\n",
      "✅ Epoch 1 | Iteration 2700 | Batch Loss: 0.4111497402191162\n",
      "✅ Epoch 1 | Iteration 2720 | Batch Loss: 0.3447209596633911\n",
      "✅ Epoch 1 | Iteration 2740 | Batch Loss: 0.1912284940481186\n",
      "✅ Epoch 1 | Iteration 2760 | Batch Loss: 0.18776853382587433\n",
      "✅ Epoch 1 | Iteration 2780 | Batch Loss: 0.3259826898574829\n",
      "✅ Epoch 1 | Iteration 2800 | Batch Loss: 0.26920485496520996\n",
      "✅ Epoch 1 | Iteration 2820 | Batch Loss: 0.2925988733768463\n",
      "✅ Epoch 1 | Iteration 2840 | Batch Loss: 0.32665783166885376\n",
      "✅ Epoch 1 | Iteration 2860 | Batch Loss: 0.3114855885505676\n",
      "✅ Epoch 1 | Iteration 2880 | Batch Loss: 0.25606244802474976\n",
      "✅ Epoch 1 | Iteration 2900 | Batch Loss: 0.2267862856388092\n",
      "✅ Epoch 1 | Iteration 2920 | Batch Loss: 0.6348995566368103\n",
      "✅ Epoch 1 | Iteration 2940 | Batch Loss: 0.2580879032611847\n",
      "✅ Epoch 1 | Iteration 2960 | Batch Loss: 0.26874372363090515\n",
      "✅ Epoch 1 | Iteration 2980 | Batch Loss: 0.13229627907276154\n",
      "✅ Epoch 1 | Iteration 3000 | Batch Loss: 0.3781820237636566\n",
      "✅ Epoch 1 | Iteration 3020 | Batch Loss: 0.5918582677841187\n",
      "✅ Epoch 1 | Iteration 3040 | Batch Loss: 0.19762307405471802\n",
      "✅ Epoch 1 | Iteration 3060 | Batch Loss: 0.3047199547290802\n",
      "✅ Epoch 1 | Iteration 3080 | Batch Loss: 0.16911311447620392\n",
      "✅ Epoch 1 | Iteration 3100 | Batch Loss: 0.3312808573246002\n",
      "✅ Epoch 1 | Iteration 3120 | Batch Loss: 0.22334809601306915\n",
      "✅ Epoch 1 | Iteration 3140 | Batch Loss: 0.2880900502204895\n",
      "✅ Epoch 1 | Iteration 3160 | Batch Loss: 0.48047924041748047\n",
      "✅ Epoch 1 | Iteration 3180 | Batch Loss: 0.19614748656749725\n",
      "✅ Epoch 1 | Iteration 3200 | Batch Loss: 0.20609107613563538\n",
      "✅ Epoch 1 | Iteration 3220 | Batch Loss: 0.3321329951286316\n",
      "✅ Epoch 1 | Iteration 3240 | Batch Loss: 0.4832092523574829\n",
      "✅ Epoch 1 | Iteration 3260 | Batch Loss: 0.33685481548309326\n",
      "✅ Epoch 1 | Iteration 3280 | Batch Loss: 0.22031737864017487\n",
      "✅ Epoch 1 | Iteration 3300 | Batch Loss: 0.12204122543334961\n",
      "✅ Epoch 1 | Iteration 3320 | Batch Loss: 0.2805735468864441\n",
      "✅ Epoch 1 | Iteration 3340 | Batch Loss: 0.3437610864639282\n",
      "✅ Epoch 1 | Iteration 3360 | Batch Loss: 0.27080076932907104\n",
      "✅ Epoch 1 | Iteration 3380 | Batch Loss: 0.5428349375724792\n",
      "✅ Epoch 1 | Iteration 3400 | Batch Loss: 0.20341095328330994\n",
      "✅ Epoch 1 | Iteration 3420 | Batch Loss: 0.2708376348018646\n",
      "✅ Epoch 1 | Iteration 3440 | Batch Loss: 0.16202972829341888\n",
      "✅ Epoch 1 | Iteration 3460 | Batch Loss: 0.34777718782424927\n",
      "✅ Epoch 1 | Iteration 3480 | Batch Loss: 0.2677517533302307\n",
      "✅ Epoch 1 Completed | Total Loss: 1036.5523408688605\n",
      "✅ Saving the best model\n",
      "✅ Epoch 1 | Validation Loss: 283.84596955776215\n",
      "✅ Epoch 2 | Iteration 0 | Batch Loss: 0.20212242007255554\n",
      "✅ Epoch 2 | Iteration 20 | Batch Loss: 0.22603467106819153\n",
      "✅ Epoch 2 | Iteration 40 | Batch Loss: 0.12095661461353302\n",
      "✅ Epoch 2 | Iteration 60 | Batch Loss: 0.3106098771095276\n",
      "✅ Epoch 2 | Iteration 80 | Batch Loss: 0.4297037124633789\n",
      "✅ Epoch 2 | Iteration 100 | Batch Loss: 0.19956620037555695\n",
      "✅ Epoch 2 | Iteration 120 | Batch Loss: 0.24705684185028076\n",
      "✅ Epoch 2 | Iteration 140 | Batch Loss: 0.3017940819263458\n",
      "✅ Epoch 2 | Iteration 160 | Batch Loss: 0.2954972982406616\n",
      "✅ Epoch 2 | Iteration 180 | Batch Loss: 0.3049543499946594\n",
      "✅ Epoch 2 | Iteration 200 | Batch Loss: 0.23750685155391693\n",
      "✅ Epoch 2 | Iteration 220 | Batch Loss: 0.1417272537946701\n",
      "✅ Epoch 2 | Iteration 240 | Batch Loss: 0.3054196536540985\n",
      "✅ Epoch 2 | Iteration 260 | Batch Loss: 0.26667433977127075\n",
      "✅ Epoch 2 | Iteration 280 | Batch Loss: 0.15409047901630402\n",
      "✅ Epoch 2 | Iteration 300 | Batch Loss: 0.32867541909217834\n",
      "✅ Epoch 2 | Iteration 320 | Batch Loss: 0.4383071959018707\n",
      "✅ Epoch 2 | Iteration 340 | Batch Loss: 0.23648601770401\n",
      "✅ Epoch 2 | Iteration 360 | Batch Loss: 0.3001154959201813\n",
      "✅ Epoch 2 | Iteration 380 | Batch Loss: 0.1738353669643402\n",
      "✅ Epoch 2 | Iteration 400 | Batch Loss: 0.1789865642786026\n",
      "✅ Epoch 2 | Iteration 420 | Batch Loss: 0.3682648837566376\n",
      "✅ Epoch 2 | Iteration 440 | Batch Loss: 0.2484370619058609\n",
      "✅ Epoch 2 | Iteration 460 | Batch Loss: 0.28749915957450867\n",
      "✅ Epoch 2 | Iteration 480 | Batch Loss: 0.22779566049575806\n",
      "✅ Epoch 2 | Iteration 500 | Batch Loss: 0.2949179410934448\n",
      "✅ Epoch 2 | Iteration 520 | Batch Loss: 0.16258859634399414\n",
      "✅ Epoch 2 | Iteration 540 | Batch Loss: 0.48640236258506775\n",
      "✅ Epoch 2 | Iteration 560 | Batch Loss: 0.24893595278263092\n",
      "✅ Epoch 2 | Iteration 580 | Batch Loss: 0.8125926852226257\n",
      "✅ Epoch 2 | Iteration 600 | Batch Loss: 0.3240405321121216\n",
      "✅ Epoch 2 | Iteration 620 | Batch Loss: 0.15503934025764465\n",
      "✅ Epoch 2 | Iteration 640 | Batch Loss: 0.23902568221092224\n",
      "✅ Epoch 2 | Iteration 660 | Batch Loss: 0.11775624752044678\n",
      "✅ Epoch 2 | Iteration 680 | Batch Loss: 0.15598008036613464\n",
      "✅ Epoch 2 | Iteration 700 | Batch Loss: 0.4336663782596588\n",
      "✅ Epoch 2 | Iteration 720 | Batch Loss: 0.4159749746322632\n",
      "✅ Epoch 2 | Iteration 740 | Batch Loss: 0.17300574481487274\n",
      "✅ Epoch 2 | Iteration 760 | Batch Loss: 0.2918024957180023\n",
      "✅ Epoch 2 | Iteration 780 | Batch Loss: 0.27613332867622375\n",
      "✅ Epoch 2 | Iteration 800 | Batch Loss: 0.28558263182640076\n",
      "✅ Epoch 2 | Iteration 820 | Batch Loss: 0.5117123126983643\n",
      "✅ Epoch 2 | Iteration 840 | Batch Loss: 0.1782865822315216\n",
      "✅ Epoch 2 | Iteration 860 | Batch Loss: 0.5302540063858032\n",
      "✅ Epoch 2 | Iteration 880 | Batch Loss: 0.3888518512248993\n",
      "✅ Epoch 2 | Iteration 900 | Batch Loss: 0.4435667395591736\n",
      "✅ Epoch 2 | Iteration 920 | Batch Loss: 0.2887732982635498\n",
      "✅ Epoch 2 | Iteration 940 | Batch Loss: 0.34854286909103394\n",
      "✅ Epoch 2 | Iteration 960 | Batch Loss: 0.12094825506210327\n",
      "✅ Epoch 2 | Iteration 980 | Batch Loss: 0.45991000533103943\n",
      "✅ Epoch 2 | Iteration 1000 | Batch Loss: 0.29040923714637756\n",
      "✅ Epoch 2 | Iteration 1020 | Batch Loss: 0.11449860036373138\n",
      "✅ Epoch 2 | Iteration 1040 | Batch Loss: 0.2167218178510666\n",
      "✅ Epoch 2 | Iteration 1060 | Batch Loss: 0.22074778378009796\n",
      "✅ Epoch 2 | Iteration 1080 | Batch Loss: 0.11865300685167313\n",
      "✅ Epoch 2 | Iteration 1100 | Batch Loss: 0.3718366026878357\n",
      "✅ Epoch 2 | Iteration 1120 | Batch Loss: 0.13469433784484863\n",
      "✅ Epoch 2 | Iteration 1140 | Batch Loss: 0.544487476348877\n",
      "✅ Epoch 2 | Iteration 1160 | Batch Loss: 0.2302401065826416\n",
      "✅ Epoch 2 | Iteration 1180 | Batch Loss: 0.3439941704273224\n",
      "✅ Epoch 2 | Iteration 1200 | Batch Loss: 0.3543340563774109\n",
      "✅ Epoch 2 | Iteration 1220 | Batch Loss: 0.17314240336418152\n",
      "✅ Epoch 2 | Iteration 1240 | Batch Loss: 0.19410797953605652\n",
      "✅ Epoch 2 | Iteration 1260 | Batch Loss: 0.07405948638916016\n",
      "✅ Epoch 2 | Iteration 1280 | Batch Loss: 0.405471533536911\n",
      "✅ Epoch 2 | Iteration 1300 | Batch Loss: 0.29930731654167175\n",
      "✅ Epoch 2 | Iteration 1320 | Batch Loss: 0.30364280939102173\n",
      "✅ Epoch 2 | Iteration 1340 | Batch Loss: 0.3033781349658966\n",
      "✅ Epoch 2 | Iteration 1360 | Batch Loss: 0.33285170793533325\n",
      "✅ Epoch 2 | Iteration 1380 | Batch Loss: 0.2614066004753113\n",
      "✅ Epoch 2 | Iteration 1400 | Batch Loss: 0.36895751953125\n",
      "✅ Epoch 2 | Iteration 1420 | Batch Loss: 0.2606014311313629\n",
      "✅ Epoch 2 | Iteration 1440 | Batch Loss: 0.17397262156009674\n",
      "✅ Epoch 2 | Iteration 1460 | Batch Loss: 0.17476287484169006\n",
      "✅ Epoch 2 | Iteration 1480 | Batch Loss: 0.17471249401569366\n",
      "✅ Epoch 2 | Iteration 1500 | Batch Loss: 0.29197362065315247\n",
      "✅ Epoch 2 | Iteration 1520 | Batch Loss: 0.20544829964637756\n",
      "✅ Epoch 2 | Iteration 1540 | Batch Loss: 0.42606228590011597\n",
      "✅ Epoch 2 | Iteration 1560 | Batch Loss: 0.34502339363098145\n",
      "✅ Epoch 2 | Iteration 1580 | Batch Loss: 0.24312251806259155\n",
      "✅ Epoch 2 | Iteration 1600 | Batch Loss: 0.20533430576324463\n",
      "✅ Epoch 2 | Iteration 1620 | Batch Loss: 0.3423009514808655\n",
      "✅ Epoch 2 | Iteration 1640 | Batch Loss: 0.3436966836452484\n",
      "✅ Epoch 2 | Iteration 1660 | Batch Loss: 0.2111406922340393\n",
      "✅ Epoch 2 | Iteration 1680 | Batch Loss: 0.28491297364234924\n",
      "✅ Epoch 2 | Iteration 1700 | Batch Loss: 0.24278534948825836\n",
      "✅ Epoch 2 | Iteration 1720 | Batch Loss: 0.12171924114227295\n",
      "✅ Epoch 2 | Iteration 1740 | Batch Loss: 0.30755558609962463\n",
      "✅ Epoch 2 | Iteration 1760 | Batch Loss: 0.2405020147562027\n",
      "✅ Epoch 2 | Iteration 1780 | Batch Loss: 0.26829037070274353\n",
      "✅ Epoch 2 | Iteration 1800 | Batch Loss: 0.14730943739414215\n",
      "✅ Epoch 2 | Iteration 1820 | Batch Loss: 0.380628377199173\n",
      "✅ Epoch 2 | Iteration 1840 | Batch Loss: 0.13671894371509552\n",
      "✅ Epoch 2 | Iteration 1860 | Batch Loss: 0.24151870608329773\n",
      "✅ Epoch 2 | Iteration 1880 | Batch Loss: 0.35419386625289917\n",
      "✅ Epoch 2 | Iteration 1900 | Batch Loss: 0.23349891602993011\n",
      "✅ Epoch 2 | Iteration 1920 | Batch Loss: 0.27743735909461975\n",
      "✅ Epoch 2 | Iteration 1940 | Batch Loss: 0.22541290521621704\n",
      "✅ Epoch 2 | Iteration 1960 | Batch Loss: 0.06747996062040329\n",
      "✅ Epoch 2 | Iteration 1980 | Batch Loss: 0.2879585027694702\n",
      "✅ Epoch 2 | Iteration 2000 | Batch Loss: 0.37430304288864136\n",
      "✅ Epoch 2 | Iteration 2020 | Batch Loss: 0.12827204167842865\n",
      "✅ Epoch 2 | Iteration 2040 | Batch Loss: 0.3542385399341583\n",
      "✅ Epoch 2 | Iteration 2060 | Batch Loss: 0.38825732469558716\n",
      "✅ Epoch 2 | Iteration 2080 | Batch Loss: 0.22968433797359467\n",
      "✅ Epoch 2 | Iteration 2100 | Batch Loss: 0.23643332719802856\n",
      "✅ Epoch 2 | Iteration 2120 | Batch Loss: 0.20982883870601654\n",
      "✅ Epoch 2 | Iteration 2140 | Batch Loss: 0.36396145820617676\n",
      "✅ Epoch 2 | Iteration 2160 | Batch Loss: 0.31144484877586365\n",
      "✅ Epoch 2 | Iteration 2180 | Batch Loss: 0.15151329338550568\n",
      "✅ Epoch 2 | Iteration 2200 | Batch Loss: 0.1917119175195694\n",
      "✅ Epoch 2 | Iteration 2220 | Batch Loss: 0.10468028485774994\n",
      "✅ Epoch 2 | Iteration 2240 | Batch Loss: 0.33400365710258484\n",
      "✅ Epoch 2 | Iteration 2260 | Batch Loss: 0.2975681722164154\n",
      "✅ Epoch 2 | Iteration 2280 | Batch Loss: 0.31381240487098694\n",
      "✅ Epoch 2 | Iteration 2300 | Batch Loss: 0.24720706045627594\n",
      "✅ Epoch 2 | Iteration 2320 | Batch Loss: 0.16389694809913635\n",
      "✅ Epoch 2 | Iteration 2340 | Batch Loss: 0.4056140184402466\n",
      "✅ Epoch 2 | Iteration 2360 | Batch Loss: 0.3141331970691681\n",
      "✅ Epoch 2 | Iteration 2380 | Batch Loss: 0.3075655996799469\n",
      "✅ Epoch 2 | Iteration 2400 | Batch Loss: 0.24349942803382874\n",
      "✅ Epoch 2 | Iteration 2420 | Batch Loss: 0.2722289562225342\n",
      "✅ Epoch 2 | Iteration 2440 | Batch Loss: 0.2562653124332428\n",
      "✅ Epoch 2 | Iteration 2460 | Batch Loss: 0.44573038816452026\n",
      "✅ Epoch 2 | Iteration 2480 | Batch Loss: 0.5210637450218201\n",
      "✅ Epoch 2 | Iteration 2500 | Batch Loss: 0.3687342405319214\n",
      "✅ Epoch 2 | Iteration 2520 | Batch Loss: 0.27629393339157104\n",
      "✅ Epoch 2 | Iteration 2540 | Batch Loss: 0.24696415662765503\n",
      "✅ Epoch 2 | Iteration 2560 | Batch Loss: 0.4269635081291199\n",
      "✅ Epoch 2 | Iteration 2580 | Batch Loss: 0.17181609570980072\n",
      "✅ Epoch 2 | Iteration 2600 | Batch Loss: 0.44010236859321594\n",
      "✅ Epoch 2 | Iteration 2620 | Batch Loss: 0.24735131859779358\n",
      "✅ Epoch 2 | Iteration 2640 | Batch Loss: 0.2690490484237671\n",
      "✅ Epoch 2 | Iteration 2660 | Batch Loss: 0.40461310744285583\n",
      "✅ Epoch 2 | Iteration 2680 | Batch Loss: 0.371291846036911\n",
      "✅ Epoch 2 | Iteration 2700 | Batch Loss: 0.4580164849758148\n",
      "✅ Epoch 2 | Iteration 2720 | Batch Loss: 0.2267502397298813\n",
      "✅ Epoch 2 | Iteration 2740 | Batch Loss: 0.30532050132751465\n",
      "✅ Epoch 2 | Iteration 2760 | Batch Loss: 0.21967455744743347\n",
      "✅ Epoch 2 | Iteration 2780 | Batch Loss: 0.18883052468299866\n",
      "✅ Epoch 2 | Iteration 2800 | Batch Loss: 0.3608590066432953\n",
      "✅ Epoch 2 | Iteration 2820 | Batch Loss: 0.324214369058609\n",
      "✅ Epoch 2 | Iteration 2840 | Batch Loss: 0.35875454545021057\n",
      "✅ Epoch 2 | Iteration 2860 | Batch Loss: 0.24775704741477966\n",
      "✅ Epoch 2 | Iteration 2880 | Batch Loss: 0.21307450532913208\n",
      "✅ Epoch 2 | Iteration 3000 | Batch Loss: 0.3444932699203491\n",
      "✅ Epoch 2 | Iteration 3020 | Batch Loss: 0.356487512588501\n",
      "✅ Epoch 2 | Iteration 3040 | Batch Loss: 0.36256542801856995\n",
      "✅ Epoch 2 | Iteration 3060 | Batch Loss: 0.37755057215690613\n",
      "✅ Epoch 2 | Iteration 3080 | Batch Loss: 0.5093119740486145\n",
      "✅ Epoch 2 | Iteration 3100 | Batch Loss: 0.391190767288208\n",
      "✅ Epoch 2 | Iteration 3120 | Batch Loss: 0.2561153769493103\n",
      "✅ Epoch 2 | Iteration 3140 | Batch Loss: 0.5443528890609741\n",
      "✅ Epoch 2 | Iteration 3160 | Batch Loss: 0.21235045790672302\n",
      "✅ Epoch 2 | Iteration 3180 | Batch Loss: 0.40149858593940735\n",
      "✅ Epoch 2 | Iteration 3200 | Batch Loss: 0.22169992327690125\n",
      "✅ Epoch 2 | Iteration 3220 | Batch Loss: 0.17024840414524078\n",
      "✅ Epoch 2 | Iteration 3240 | Batch Loss: 0.2849637567996979\n",
      "✅ Epoch 2 | Iteration 3260 | Batch Loss: 0.3292694389820099\n",
      "✅ Epoch 2 | Iteration 3280 | Batch Loss: 0.25367698073387146\n",
      "✅ Epoch 2 | Iteration 3300 | Batch Loss: 0.3281986713409424\n",
      "✅ Epoch 2 | Iteration 3320 | Batch Loss: 0.3277989625930786\n",
      "✅ Epoch 2 | Iteration 3340 | Batch Loss: 0.3853604793548584\n",
      "✅ Epoch 2 | Iteration 3360 | Batch Loss: 0.17971192300319672\n",
      "✅ Epoch 2 | Iteration 3380 | Batch Loss: 0.3625122904777527\n",
      "✅ Epoch 2 | Iteration 3400 | Batch Loss: 0.2652604579925537\n",
      "✅ Epoch 2 | Iteration 3420 | Batch Loss: 0.3632590174674988\n",
      "✅ Epoch 2 | Iteration 3440 | Batch Loss: 0.3285699784755707\n",
      "✅ Epoch 2 | Iteration 3460 | Batch Loss: 0.5016477704048157\n",
      "✅ Epoch 2 | Iteration 3480 | Batch Loss: 0.23670926690101624\n",
      "✅ Epoch 2 Completed | Total Loss: 1005.295132625848\n"
     ]
    }
   ],
   "source": [
    "# ✅ Training Loop\n",
    "num_epochs = 3\n",
    "best_validation_loss = float(\"inf\")\n",
    "validation_loss = []\n",
    "training_loss = []\n",
    "training_loss_dict = []\n",
    "validation_loss_dict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ✅ Train\n",
    "    loss_dict, loss = train_one_epoch(model, data_loader, optimizer, device, epoch)\n",
    "    training_loss.append(loss)\n",
    "    training_loss_dict.append(loss_dict)\n",
    "\n",
    "    # ✅ Step Scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # ✅ Validation\n",
    "    loss_dict, loss = calculate_validation_loss(model, data_loader_val, device)\n",
    "    validation_loss.append(loss)\n",
    "    validation_loss_dict.append(loss_dict)\n",
    "\n",
    "    # ✅ Save Best Model\n",
    "    if loss < best_validation_loss:\n",
    "        best_validation_loss = loss\n",
    "        print(\"✅ Saving the best model\")\n",
    "        torch.save(model.state_dict(), output_dir + f\"best_{epoch}.pt\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch} | Validation Loss: {loss}\")\n",
    "\n",
    "# ✅ Save Loss Logs\n",
    "with open(output_dir + 'loss_info.txt', 'w') as f:\n",
    "    for i, (tloss, vloss) in enumerate(zip(training_loss, validation_loss)):\n",
    "        f.write(f\"Epoch {i}: Training Loss: {tloss} | Validation Loss: {vloss}\\n\")\n",
    "\n",
    "# ✅ Test Predictions\n",
    "predictions = evaluate(model, data_loader_test, device)\n",
    "print(f\"✅ Predictions made on {len(predictions)} images\")\n",
    "\n",
    "# ✅ Save Test Predictions\n",
    "with open(output_dir + \"test.txt\", 'w') as f:\n",
    "    for img_id, values in predictions.items():\n",
    "        img_name = str(img_id + 1).zfill(5) + \".jpeg\"\n",
    "\n",
    "        # ✅ Use PIL instead of cv2\n",
    "        with Image.open(root + \"test/images/\" + img_name) as img:\n",
    "            w, h = img.size  # PIL gives (width, height)\n",
    "\n",
    "        # ✅ Apply NMS\n",
    "        '''labels = values['labels']\n",
    "        boxes = values['boxes']\n",
    "        scores = values['scores']\n",
    "        indices = nms(boxes, scores, 0.8)\n",
    "        for i in indices:'''\n",
    "\n",
    "        for label, bbox, score in zip(values['labels'], values['boxes'], values['scores']):\n",
    "            box = convert2Outputbox(bbox, w, h)\n",
    "            f.write(f\"{img_id+1} {label} {box[0]} {box[1]} {box[2]} {box[3]} {score}\\n\")\n",
    "\n",
    "print(\"✅ Test predictions saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb150f-80c1-4620-b9b0-c4a7cdec9aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssh",
   "language": "python",
   "name": "ssh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
